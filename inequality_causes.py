# -*- coding: utf-8 -*-
"""inequality_causes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQU8dD6k0b_qziBVkit3OnmY1yfUu48a
"""

# input:Corpus.txt
# output: top_causes.csv

import nltk

required_resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']
for resource in required_resources:
    try:
        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else resource)
    except LookupError:
        print(f"Загружаю ресурс '{resource}'...")
        nltk.download(resource)

import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
import pandas as pd
import re

with open('Corpus.txt', mode='r', encoding='utf-8') as f:
    text = f.read()

text = text.lower()

try:
    sentences = sent_tokenize(text)
except Exception as e:
    print(f"Tokenization error: {e}")
    raise

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
custom_stopwords = {'figure'}
stop_words.update(custom_stopwords)

ineq_keywords = {
    "inequality", "inequity", "disparity", "gap",
    "poverty", "deprivation", "income", "wealth",
    "redistribution", "mobility", "segregation"
}

cause_patterns = [
    r"\bbecause of\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bdue to\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bas a result of\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bresult(?:s)? of\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bdriven by\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bcaused by\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\battributed to\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\bstemming from\s+([^.;:,\n\(\)\[\]]{5,200})",
    r"\barising from\s+([^.;:,\n\(\)\[\]]{5,200})",
]

def normalize_cause_fragment(fragment: str) -> str:
    fragment = " ".join(fragment.strip().split()[:12])

    toks = word_tokenize(fragment)
    clean = [
        lemmatizer.lemmatize(t.lower())
        for t in toks
        if t.isalpha() and len(t) > 2 and t.lower() not in stop_words
    ]
    if len(clean) < 2:
        return ""
    return " ".join(clean)

cause_phrases = []
cause_examples = {}

for s in sentences:
    s_low = s.lower()
    if not any(k in s_low for k in ineq_keywords):
        continue

    for rx in cause_patterns:
        for m in re.finditer(rx, s_low):
            frag = m.group(1)
            frag = re.split(r"\b(and|but|while|whereas|which)\b", frag, maxsplit=1)[0]
            norm = normalize_cause_fragment(frag)

            if norm:
                cause_phrases.append(norm)
                if norm not in cause_examples:
                    cause_examples[norm] = s.strip()

if cause_phrases:
    cause_fdist = FreqDist(cause_phrases)
    top_causes = cause_fdist.most_common(30)

    causes, counts = zip(*top_causes)

    plt.figure(figsize=(20, 10))
    sns.barplot(x=list(counts), y=list(causes), orient='h')
    plt.title("Top-30 causes (from causal statements about inequality)")
    plt.xlabel("Frequency")
    plt.ylabel("Cause")
    plt.tight_layout()
    plt.savefig("top_causes.png", dpi=170)
    plt.close()
    print("Saved: top_causes.png")

    for c, n in top_causes[:15]:
        print(f"{n:3d}  {c}\n     e.g. {cause_examples.get(c, '')}\n")

    df_causes = pd.DataFrame([
        {"cause": c, "count": n, "example": cause_examples.get(c, "")}
        for c, n in top_causes
    ])
else:
    print("No causal reasons found with current patterns.")
    df_causes = pd.DataFrame(columns=["cause", "count", "example"])
    
df_causes.to_csv("top_causes.csv", index=False)
print("Saved: top_causes.csv")
